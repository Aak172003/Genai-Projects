{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -q -U google-generativeai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "from dotenv import load_dotenv  # helps to load env files\n",
    "\n",
    "import google.generativeai as genai\n",
    "# Load all env files\n",
    "load_dotenv()\n",
    "\n",
    "GOOGLE_API_KEY = os.getenv('GOOGLE_API_KEY')\n",
    "\n",
    "os.environ['GOOGLE_API_KEY'] = os.getenv('GOOGLE_API_KEY')\n",
    "genai.configure(api_key=GOOGLE_API_KEY)\n",
    "GOOGLE_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(genai.list_models())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform Embedding on single string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "\n",
    "result :Dict = genai.embed_content(\n",
    "    model=\"models/text-embedding-004\",\n",
    "    # This content will goona convert into embedding \n",
    "    content=\"What is the meaning of life?\",\n",
    "    task_type=\"retrieval_document\",\n",
    "    title=\"Embedding of single string\"\n",
    ")\n",
    "\n",
    "result['embedding']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(result['embedding'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform Embedding on list of string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "result:Dict = genai.embed_content(\n",
    "    model=\"models/text-embedding-004\",\n",
    "    content=[\n",
    "        'what is meaning of life?',\n",
    "        \"How does the brain work?\",\n",
    "        \"Explain difference between CPU and GPU?\"\n",
    "    ],\n",
    "    task_type=\"retrieval_document\",\n",
    "    title=\"Embedding of list of string\"\n",
    "\n",
    ")\n",
    "\n",
    "\n",
    "len(result['embedding'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for embedded_result in result['embedding']:\n",
    "    print(str(embedded_result)[:50], \" ..... Trimmed ...... \", len(embedded_result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building Vector Store & Reterival using Chroma DB and LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -Uq langchain-chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "\n",
    "documents = [\n",
    "    Document(\n",
    "        page_content=\"Roses are one of the most popular flowers, symbolizing love and passion. They come in various colors, each carrying different meanings. Proper care includes regular watering, pruning, and exposure to sunlight.\",\n",
    "        metadata={\"source\": \"flower-garden-docs\"}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Tulips are vibrant spring flowers that bloom in a variety of colors. They thrive in well-drained soil and require full sun to partial shade. Tulips are often associated with elegance and new beginnings.\",\n",
    "        metadata={\"source\": \"flower-garden-docs\"}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Lotuses are aquatic flowers that symbolize purity and enlightenment. They grow in ponds and still waters, requiring warm temperatures and full sunlight. Their leaves repel water, keeping them dry even in wet conditions.\",\n",
    "        metadata={\"source\": \"aquatic-flowers-docs\"}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Sunflowers are known for their large, bright yellow blooms that follow the sun. They require direct sunlight and well-drained soil to thrive. Sunflowers symbolize happiness, warmth, and positivity.\",\n",
    "        metadata={\"source\": \"field-flowers-docs\"}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Orchids are exotic flowers admired for their intricate beauty and variety. They require a humid environment, indirect sunlight, and well-ventilated spaces. Orchids symbolize love, strength, and refinement.\",\n",
    "        metadata={\"source\": \"flower-garden-docs\"}\n",
    "    ),\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -Uq langchain-google-genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "\n",
    "embeddings = GoogleGenerativeAIEmbeddings(\n",
    "    model=\"models/embedding-001\" , \n",
    "    google_api_key=os.getenv('GOOGLE_API_KEY')\n",
    ")\n",
    "\n",
    "embeddings , os.getenv('GOOGLE_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = embeddings.embed_query(\"Rose Color\")\n",
    "\n",
    "len(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here it use google gemini embedding model\n",
    "\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "vector_store = Chroma.from_documents(documents , embedding=embeddings)\n",
    "\n",
    "vector_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(dir(vector_store))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store.similarity_search(\"Tulip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await vector_store.asimilarity_search(\"tulip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store.similarity_search_with_score(\"tulip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similarity Serach By Vector\n",
    "new_embd = embeddings.embed_query(\"Tulip\")\n",
    "\n",
    "# we can also search by vector \n",
    "vector_store.similarity_search_by_vector(new_embd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrivers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# when i run query at that time its object will create and use and then automatically remove that object from memory\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "# Return top most whose cosine angle are closer to each other \n",
    "reteriver = RunnableLambda(vector_store.similarity_search).bind(k=1)\n",
    "\n",
    "reteriver.batch(['tulip']) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\", api_key= GOOGLE_API_KEY)\n",
    "\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# llm give huge response but our rag return minimum response , which is exist in my context window \n",
    "llm_response = llm.invoke(\"Tell me variety of tulip ? and where i can get tulip and also tell me tulip flower process?\")\n",
    "\n",
    "print(llm_response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# This runnablePassThrough help to directly pass our question to llms and\n",
    "# which means we are make dynamically prompt template so  it means when question resceive it will dynamically add into query \n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "\n",
    "\n",
    "message = \"\"\"\n",
    "    You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. \n",
    "    If you don't know the answer, just say that you don't know.\n",
    "    If possible can u please give me answer in bullet points . Make sure your answer is relevent to the question ans it is answered from the contetx only.\n",
    "    Question: {question} \n",
    "    Context: {context} \n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([(\"human\" , message)])\n",
    "prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare RAG Chain or RAG Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import output parser \n",
    "# with the help of Stroutputparser we can able to get final respone in string format \n",
    "from langchain_core.output_parsers import StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain = {\"context\":reteriver , \"question\":RunnablePassthrough()} | prompt | llm | StrOutputParser()\n",
    "\n",
    "rag_chain\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# question = \"Tell me variety of tulip ? and where i can get tulip and also tell me tulip flower process?\"\n",
    "\n",
    "# question=\"Tell me about Aakash Prajapati\"\n",
    "question=\"Tell me about shark\"\n",
    "response = rag_chain.invoke(question)\n",
    "\n",
    "# llm give huge response but our rag return minimum response , which is exist in my context window \n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
